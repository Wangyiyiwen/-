{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c28c999",
   "metadata": {},
   "source": [
    "# 汇率预测机器人"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78aef093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 09:56:40.991989: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-10 09:56:41.011312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752112601.030694   53524 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752112601.037015   53524 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752112601.052558   53524 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752112601.052595   53524 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752112601.052596   53524 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752112601.052598   53524 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-10 09:56:41.058255: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv, DataFrame, concat\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "class CurrencyLSTMModel:\n",
    "    def __init__(self, currency_name, data_path, look_back=1):\n",
    "        self.currency_name = currency_name\n",
    "        self.data_path = data_path\n",
    "        self.look_back = look_back\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.model = None\n",
    "\n",
    "    def load_and_prepare_data(self):\n",
    "     dataset = read_csv(self.data_path, header=0, index_col=0)\n",
    "    # 直接用全部数据（只有汇率一列）\n",
    "     values = dataset.values.astype('float32')\n",
    "     scaled = self.scaler.fit_transform(values)\n",
    "     reframed = series_to_supervised(scaled, self.look_back, 1)\n",
    "     values = reframed.values\n",
    "     n_train = int(len(values) * 0.7)\n",
    "     train = values[:n_train, :]\n",
    "     test = values[n_train:, :]\n",
    "     train_X, train_y = train[:, :-1], train[:, -1]\n",
    "     test_X, test_y = test[:, :-1], test[:, -1]\n",
    "     train_X = train_X.reshape((train_X.shape[0],train_X.shape[1],1))\n",
    "     test_X = test_X.reshape((test_X.shape[0],test_X.shape[1],1))\n",
    "     return train_X, train_y, test_X, test_y\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(6, input_shape=input_shape, return_sequences=True))\n",
    "        model.add(LSTM(150, return_sequences=True))\n",
    "        model.add(LSTM(50, return_sequences=False))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, epochs=100, batch_size=60):\n",
    "        train_X, train_y, test_X, test_y = self.load_and_prepare_data()\n",
    "        self.build_model((train_X.shape[1], train_X.shape[2]))\n",
    "        history = self.model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size,\n",
    "                                 validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        return history\n",
    "\n",
    "    def save(self, model_path):\n",
    "        self.model.save(model_path)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        self.model = load_model(model_path)\n",
    "\n",
    "    def predict(self, X):\n",
    "        yhat = self.model.predict(X)\n",
    "        return yhat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d902d",
   "metadata": {},
   "source": [
    "## 新币汇率预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090db30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测 JPY 对人民币的汇率：\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rprp/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 5s - 3s/step - loss: 0.3330 - val_loss: 0.3181\n",
      "Epoch 2/100\n",
      "2/2 - 1s - 284ms/step - loss: 0.2286 - val_loss: 0.1991\n",
      "Epoch 3/100\n",
      "2/2 - 1s - 258ms/step - loss: 0.1030 - val_loss: 0.2956\n",
      "Epoch 4/100\n",
      "2/2 - 0s - 229ms/step - loss: 0.2253 - val_loss: 0.1485\n",
      "Epoch 5/100\n",
      "2/2 - 0s - 225ms/step - loss: 0.1349 - val_loss: 0.1730\n",
      "Epoch 6/100\n",
      "2/2 - 0s - 234ms/step - loss: 0.1148 - val_loss: 0.1957\n",
      "Epoch 7/100\n",
      "2/2 - 0s - 243ms/step - loss: 0.1214 - val_loss: 0.1988\n",
      "Epoch 8/100\n",
      "2/2 - 1s - 251ms/step - loss: 0.1183 - val_loss: 0.1876\n",
      "Epoch 9/100\n",
      "2/2 - 1s - 255ms/step - loss: 0.1069 - val_loss: 0.1748\n",
      "Epoch 10/100\n",
      "2/2 - 1s - 251ms/step - loss: 0.1036 - val_loss: 0.1631\n",
      "Epoch 11/100\n",
      "2/2 - 0s - 247ms/step - loss: 0.1041 - val_loss: 0.1537\n",
      "Epoch 12/100\n",
      "2/2 - 0s - 233ms/step - loss: 0.1063 - val_loss: 0.1481\n",
      "Epoch 13/100\n",
      "2/2 - 0s - 237ms/step - loss: 0.1083 - val_loss: 0.1471\n",
      "Epoch 14/100\n",
      "2/2 - 0s - 220ms/step - loss: 0.1084 - val_loss: 0.1495\n",
      "Epoch 15/100\n",
      "2/2 - 1s - 252ms/step - loss: 0.1069 - val_loss: 0.1540\n",
      "Epoch 16/100\n",
      "2/2 - 1s - 281ms/step - loss: 0.1053 - val_loss: 0.1589\n",
      "Epoch 17/100\n",
      "2/2 - 0s - 243ms/step - loss: 0.1037 - val_loss: 0.1631\n",
      "Epoch 18/100\n",
      "2/2 - 0s - 238ms/step - loss: 0.1025 - val_loss: 0.1662\n",
      "Epoch 19/100\n",
      "2/2 - 1s - 256ms/step - loss: 0.1016 - val_loss: 0.1679\n",
      "Epoch 20/100\n",
      "2/2 - 0s - 239ms/step - loss: 0.1010 - val_loss: 0.1679\n",
      "Epoch 21/100\n",
      "2/2 - 1s - 268ms/step - loss: 0.1002 - val_loss: 0.1661\n",
      "Epoch 22/100\n",
      "2/2 - 0s - 230ms/step - loss: 0.0992 - val_loss: 0.1627\n",
      "Epoch 23/100\n",
      "2/2 - 0s - 236ms/step - loss: 0.0984 - val_loss: 0.1587\n",
      "Epoch 24/100\n",
      "2/2 - 0s - 240ms/step - loss: 0.0978 - val_loss: 0.1545\n",
      "Epoch 25/100\n",
      "2/2 - 0s - 243ms/step - loss: 0.0971 - val_loss: 0.1501\n",
      "Epoch 26/100\n",
      "2/2 - 0s - 233ms/step - loss: 0.0965 - val_loss: 0.1466\n",
      "Epoch 27/100\n",
      "2/2 - 0s - 238ms/step - loss: 0.0958 - val_loss: 0.1439\n",
      "Epoch 28/100\n",
      "2/2 - 0s - 249ms/step - loss: 0.0948 - val_loss: 0.1420\n",
      "Epoch 29/100\n",
      "2/2 - 1s - 270ms/step - loss: 0.0935 - val_loss: 0.1408\n",
      "Epoch 30/100\n",
      "2/2 - 0s - 223ms/step - loss: 0.0918 - val_loss: 0.1398\n",
      "Epoch 31/100\n",
      "2/2 - 0s - 218ms/step - loss: 0.0898 - val_loss: 0.1388\n",
      "Epoch 32/100\n",
      "2/2 - 0s - 214ms/step - loss: 0.0875 - val_loss: 0.1382\n",
      "Epoch 33/100\n",
      "2/2 - 0s - 213ms/step - loss: 0.0846 - val_loss: 0.1355\n",
      "Epoch 34/100\n",
      "2/2 - 1s - 357ms/step - loss: 0.0814 - val_loss: 0.1314\n",
      "Epoch 35/100\n",
      "2/2 - 1s - 250ms/step - loss: 0.0775 - val_loss: 0.1251\n",
      "Epoch 36/100\n",
      "2/2 - 1s - 253ms/step - loss: 0.0737 - val_loss: 0.1246\n",
      "Epoch 37/100\n",
      "2/2 - 1s - 261ms/step - loss: 0.0721 - val_loss: 0.1253\n",
      "Epoch 38/100\n",
      "2/2 - 0s - 235ms/step - loss: 0.0689 - val_loss: 0.1229\n",
      "Epoch 39/100\n",
      "2/2 - 0s - 236ms/step - loss: 0.0734 - val_loss: 0.1177\n",
      "Epoch 40/100\n",
      "2/2 - 0s - 238ms/step - loss: 0.0703 - val_loss: 0.1089\n",
      "Epoch 41/100\n",
      "2/2 - 1s - 503ms/step - loss: 0.0680 - val_loss: 0.1034\n",
      "Epoch 42/100\n",
      "2/2 - 0s - 240ms/step - loss: 0.0689 - val_loss: 0.0992\n",
      "Epoch 43/100\n",
      "2/2 - 0s - 236ms/step - loss: 0.0669 - val_loss: 0.0972\n",
      "Epoch 44/100\n",
      "2/2 - 0s - 240ms/step - loss: 0.0667 - val_loss: 0.0952\n",
      "Epoch 45/100\n",
      "2/2 - 0s - 250ms/step - loss: 0.0666 - val_loss: 0.0909\n",
      "Epoch 46/100\n",
      "2/2 - 0s - 228ms/step - loss: 0.0658 - val_loss: 0.0957\n",
      "Epoch 47/100\n",
      "2/2 - 0s - 227ms/step - loss: 0.0656 - val_loss: 0.0916\n",
      "Epoch 48/100\n",
      "2/2 - 0s - 231ms/step - loss: 0.0645 - val_loss: 0.1038\n",
      "Epoch 49/100\n",
      "2/2 - 0s - 220ms/step - loss: 0.0649 - val_loss: 0.1027\n",
      "Epoch 50/100\n",
      "2/2 - 0s - 233ms/step - loss: 0.0640 - val_loss: 0.0944\n",
      "Epoch 51/100\n",
      "2/2 - 1s - 356ms/step - loss: 0.0639 - val_loss: 0.1061\n",
      "Epoch 52/100\n",
      "2/2 - 1s - 260ms/step - loss: 0.0641 - val_loss: 0.0989\n",
      "Epoch 53/100\n",
      "2/2 - 0s - 247ms/step - loss: 0.0632 - val_loss: 0.1012\n",
      "Epoch 54/100\n",
      "2/2 - 0s - 241ms/step - loss: 0.0628 - val_loss: 0.1077\n",
      "Epoch 55/100\n",
      "2/2 - 0s - 247ms/step - loss: 0.0626 - val_loss: 0.0989\n",
      "Epoch 56/100\n",
      "2/2 - 0s - 250ms/step - loss: 0.0621 - val_loss: 0.1410\n",
      "Epoch 57/100\n",
      "2/2 - 0s - 245ms/step - loss: 0.0675 - val_loss: 0.0933\n",
      "Epoch 58/100\n",
      "2/2 - 1s - 283ms/step - loss: 0.0639 - val_loss: 0.1014\n",
      "Epoch 59/100\n",
      "2/2 - 0s - 236ms/step - loss: 0.0625 - val_loss: 0.1064\n",
      "Epoch 60/100\n",
      "2/2 - 1s - 255ms/step - loss: 0.0654 - val_loss: 0.1037\n",
      "Epoch 61/100\n",
      "2/2 - 1s - 255ms/step - loss: 0.0642 - val_loss: 0.1017\n",
      "Epoch 62/100\n",
      "2/2 - 0s - 236ms/step - loss: 0.0625 - val_loss: 0.1019\n",
      "Epoch 63/100\n",
      "2/2 - 0s - 233ms/step - loss: 0.0611 - val_loss: 0.1032\n",
      "Epoch 64/100\n",
      "2/2 - 0s - 236ms/step - loss: 0.0618 - val_loss: 0.1013\n",
      "Epoch 65/100\n",
      "2/2 - 0s - 221ms/step - loss: 0.0617 - val_loss: 0.0992\n",
      "Epoch 66/100\n",
      "2/2 - 0s - 228ms/step - loss: 0.0612 - val_loss: 0.0980\n",
      "Epoch 67/100\n",
      "2/2 - 0s - 246ms/step - loss: 0.0607 - val_loss: 0.0975\n",
      "Epoch 68/100\n",
      "2/2 - 0s - 225ms/step - loss: 0.0606 - val_loss: 0.0968\n",
      "Epoch 69/100\n",
      "2/2 - 0s - 245ms/step - loss: 0.0606 - val_loss: 0.0963\n",
      "Epoch 70/100\n",
      "2/2 - 0s - 234ms/step - loss: 0.0601 - val_loss: 0.0964\n",
      "Epoch 71/100\n",
      "2/2 - 0s - 229ms/step - loss: 0.0602 - val_loss: 0.0946\n",
      "Epoch 72/100\n",
      "2/2 - 0s - 226ms/step - loss: 0.0602 - val_loss: 0.0942\n",
      "Epoch 73/100\n",
      "2/2 - 1s - 281ms/step - loss: 0.0596 - val_loss: 0.0948\n",
      "Epoch 74/100\n",
      "2/2 - 1s - 274ms/step - loss: 0.0596 - val_loss: 0.0953\n",
      "Epoch 75/100\n",
      "2/2 - 1s - 275ms/step - loss: 0.0595 - val_loss: 0.0955\n",
      "Epoch 76/100\n",
      "2/2 - 1s - 250ms/step - loss: 0.0596 - val_loss: 0.0944\n",
      "Epoch 77/100\n",
      "2/2 - 1s - 278ms/step - loss: 0.0593 - val_loss: 0.0944\n",
      "Epoch 78/100\n",
      "2/2 - 1s - 270ms/step - loss: 0.0593 - val_loss: 0.0938\n",
      "Epoch 79/100\n",
      "2/2 - 1s - 259ms/step - loss: 0.0590 - val_loss: 0.0934\n",
      "Epoch 80/100\n",
      "2/2 - 1s - 260ms/step - loss: 0.0590 - val_loss: 0.0920\n",
      "Epoch 81/100\n",
      "2/2 - 1s - 257ms/step - loss: 0.0587 - val_loss: 0.0935\n",
      "Epoch 82/100\n",
      "2/2 - 1s - 266ms/step - loss: 0.0587 - val_loss: 0.0927\n",
      "Epoch 83/100\n",
      "2/2 - 1s - 286ms/step - loss: 0.0590 - val_loss: 0.0895\n",
      "Epoch 84/100\n",
      "2/2 - 1s - 264ms/step - loss: 0.0587 - val_loss: 0.0909\n",
      "Epoch 85/100\n",
      "2/2 - 0s - 245ms/step - loss: 0.0579 - val_loss: 0.1002\n",
      "Epoch 86/100\n",
      "2/2 - 1s - 275ms/step - loss: 0.0588 - val_loss: 0.0927\n",
      "Epoch 87/100\n",
      "2/2 - 1s - 294ms/step - loss: 0.0584 - val_loss: 0.0884\n",
      "Epoch 88/100\n",
      "2/2 - 0s - 247ms/step - loss: 0.0579 - val_loss: 0.0964\n",
      "Epoch 89/100\n",
      "2/2 - 0s - 249ms/step - loss: 0.0575 - val_loss: 0.1116\n",
      "Epoch 90/100\n",
      "2/2 - 1s - 263ms/step - loss: 0.0581 - val_loss: 0.1091\n",
      "Epoch 91/100\n",
      "2/2 - 1s - 252ms/step - loss: 0.0581 - val_loss: 0.0951\n",
      "Epoch 92/100\n",
      "2/2 - 1s - 258ms/step - loss: 0.0577 - val_loss: 0.0975\n",
      "Epoch 93/100\n",
      "2/2 - 0s - 250ms/step - loss: 0.0569 - val_loss: 0.1179\n",
      "Epoch 94/100\n",
      "2/2 - 0s - 246ms/step - loss: 0.0572 - val_loss: 0.1146\n",
      "Epoch 95/100\n",
      "2/2 - 0s - 239ms/step - loss: 0.0570 - val_loss: 0.1057\n",
      "Epoch 96/100\n",
      "2/2 - 0s - 249ms/step - loss: 0.0565 - val_loss: 0.1080\n",
      "Epoch 97/100\n",
      "2/2 - 0s - 228ms/step - loss: 0.0561 - val_loss: 0.1239\n",
      "Epoch 98/100\n",
      "2/2 - 1s - 283ms/step - loss: 0.0577 - val_loss: 0.0840\n",
      "Epoch 99/100\n",
      "2/2 - 1s - 266ms/step - loss: 0.0563 - val_loss: 0.0972\n",
      "Epoch 100/100\n",
      "2/2 - 0s - 237ms/step - loss: 0.0568 - val_loss: 0.0939\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 442ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 656ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "未来20天预测汇率：\n",
      "[0.04543228 0.04536617 0.04533456 0.04533432 0.04536003 0.04540592\n",
      " 0.04546712 0.04553984 0.04562137 0.04570986 0.04580401 0.04590295\n",
      " 0.04600598 0.04611256 0.04622221 0.04633446 0.04644885 0.04656496\n",
      " 0.04668238 0.04680076]\n",
      "------------------------------\n",
      "预测 KRW 对人民币的汇率：\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rprp/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 - 4s - 2s/step - loss: 0.4439 - val_loss: 0.5372\n",
      "Epoch 2/100\n",
      "2/2 - 1s - 251ms/step - loss: 0.2651 - val_loss: 0.1462\n",
      "Epoch 3/100\n",
      "2/2 - 1s - 251ms/step - loss: 0.1027 - val_loss: 0.3719\n",
      "Epoch 4/100\n",
      "2/2 - 1s - 292ms/step - loss: 0.2250 - val_loss: 0.1286\n",
      "Epoch 5/100\n",
      "2/2 - 1s - 283ms/step - loss: 0.1311 - val_loss: 0.1304\n",
      "Epoch 6/100\n",
      "2/2 - 1s - 252ms/step - loss: 0.1473 - val_loss: 0.2054\n",
      "Epoch 7/100\n",
      "2/2 - 0s - 234ms/step - loss: 0.1541 - val_loss: 0.1846\n",
      "Epoch 8/100\n",
      "2/2 - 1s - 277ms/step - loss: 0.1343 - val_loss: 0.1121\n",
      "Epoch 9/100\n",
      "2/2 - 0s - 248ms/step - loss: 0.1075 - val_loss: 0.0698\n",
      "Epoch 10/100\n",
      "2/2 - 1s - 253ms/step - loss: 0.1055 - val_loss: 0.1238\n",
      "Epoch 11/100\n",
      "2/2 - 0s - 229ms/step - loss: 0.1221 - val_loss: 0.1172\n",
      "Epoch 12/100\n",
      "2/2 - 1s - 268ms/step - loss: 0.1112 - val_loss: 0.0699\n",
      "Epoch 13/100\n",
      "2/2 - 0s - 244ms/step - loss: 0.1048 - val_loss: 0.0732\n",
      "Epoch 14/100\n",
      "2/2 - 0s - 233ms/step - loss: 0.1051 - val_loss: 0.0819\n",
      "Epoch 15/100\n",
      "2/2 - 0s - 220ms/step - loss: 0.1043 - val_loss: 0.0717\n",
      "Epoch 16/100\n",
      "2/2 - 0s - 228ms/step - loss: 0.0987 - val_loss: 0.0672\n",
      "Epoch 17/100\n",
      "2/2 - 0s - 231ms/step - loss: 0.0920 - val_loss: 0.1199\n",
      "Epoch 18/100\n",
      "2/2 - 0s - 241ms/step - loss: 0.0967 - val_loss: 0.1670\n",
      "Epoch 19/100\n",
      "2/2 - 1s - 250ms/step - loss: 0.0974 - val_loss: 0.1318\n",
      "Epoch 20/100\n",
      "2/2 - 0s - 246ms/step - loss: 0.0890 - val_loss: 0.0789\n",
      "Epoch 21/100\n",
      "2/2 - 1s - 276ms/step - loss: 0.0914 - val_loss: 0.0664\n",
      "Epoch 22/100\n",
      "2/2 - 1s - 268ms/step - loss: 0.0918 - val_loss: 0.0769\n",
      "Epoch 23/100\n",
      "2/2 - 1s - 256ms/step - loss: 0.0880 - val_loss: 0.1160\n",
      "Epoch 24/100\n",
      "2/2 - 1s - 270ms/step - loss: 0.0868 - val_loss: 0.1523\n",
      "Epoch 25/100\n",
      "2/2 - 1s - 278ms/step - loss: 0.0875 - val_loss: 0.1499\n",
      "Epoch 26/100\n",
      "2/2 - 0s - 235ms/step - loss: 0.0853 - val_loss: 0.1283\n",
      "Epoch 27/100\n",
      "2/2 - 1s - 261ms/step - loss: 0.0857 - val_loss: 0.1222\n",
      "Epoch 28/100\n",
      "2/2 - 1s - 254ms/step - loss: 0.0858 - val_loss: 0.1344\n",
      "Epoch 29/100\n",
      "2/2 - 1s - 268ms/step - loss: 0.0855 - val_loss: 0.1488\n",
      "Epoch 30/100\n",
      "2/2 - 0s - 238ms/step - loss: 0.0849 - val_loss: 0.1470\n",
      "Epoch 31/100\n",
      "2/2 - 0s - 239ms/step - loss: 0.0842 - val_loss: 0.1322\n",
      "Epoch 32/100\n",
      "2/2 - 0s - 234ms/step - loss: 0.0838 - val_loss: 0.1240\n",
      "Epoch 33/100\n",
      "2/2 - 0s - 236ms/step - loss: 0.0836 - val_loss: 0.1302\n",
      "Epoch 34/100\n",
      "2/2 - 0s - 238ms/step - loss: 0.0831 - val_loss: 0.1354\n",
      "Epoch 35/100\n",
      "2/2 - 1s - 264ms/step - loss: 0.0826 - val_loss: 0.1294\n",
      "Epoch 36/100\n",
      "2/2 - 1s - 250ms/step - loss: 0.0819 - val_loss: 0.1153\n",
      "Epoch 37/100\n",
      "2/2 - 0s - 239ms/step - loss: 0.0816 - val_loss: 0.1095\n",
      "Epoch 38/100\n",
      "2/2 - 0s - 233ms/step - loss: 0.0812 - val_loss: 0.1145\n",
      "Epoch 39/100\n",
      "2/2 - 0s - 243ms/step - loss: 0.0806 - val_loss: 0.1200\n",
      "Epoch 40/100\n",
      "2/2 - 0s - 249ms/step - loss: 0.0799 - val_loss: 0.1215\n",
      "Epoch 41/100\n",
      "2/2 - 1s - 277ms/step - loss: 0.0792 - val_loss: 0.1199\n",
      "Epoch 42/100\n",
      "2/2 - 1s - 276ms/step - loss: 0.0783 - val_loss: 0.1162\n",
      "Epoch 43/100\n",
      "2/2 - 0s - 243ms/step - loss: 0.0774 - val_loss: 0.1115\n",
      "Epoch 44/100\n",
      "2/2 - 0s - 246ms/step - loss: 0.0765 - val_loss: 0.1173\n",
      "Epoch 45/100\n",
      "2/2 - 0s - 241ms/step - loss: 0.0756 - val_loss: 0.1270\n",
      "Epoch 46/100\n",
      "2/2 - 1s - 266ms/step - loss: 0.0741 - val_loss: 0.1291\n",
      "Epoch 47/100\n",
      "2/2 - 0s - 249ms/step - loss: 0.0724 - val_loss: 0.1461\n",
      "Epoch 48/100\n",
      "2/2 - 1s - 259ms/step - loss: 0.0713 - val_loss: 0.1338\n",
      "Epoch 49/100\n",
      "2/2 - 1s - 288ms/step - loss: 0.0744 - val_loss: 0.2313\n",
      "Epoch 50/100\n",
      "2/2 - 1s - 283ms/step - loss: 0.1001 - val_loss: 0.1418\n",
      "Epoch 51/100\n",
      "2/2 - 1s - 272ms/step - loss: 0.0920 - val_loss: 0.0750\n",
      "Epoch 52/100\n",
      "2/2 - 0s - 249ms/step - loss: 0.0792 - val_loss: 0.1217\n",
      "Epoch 53/100\n",
      "2/2 - 1s - 257ms/step - loss: 0.0843 - val_loss: 0.1824\n",
      "Epoch 54/100\n",
      "2/2 - 1s - 543ms/step - loss: 0.0833 - val_loss: 0.0864\n",
      "Epoch 55/100\n",
      "2/2 - 0s - 237ms/step - loss: 0.0722 - val_loss: 0.0650\n",
      "Epoch 56/100\n",
      "2/2 - 1s - 261ms/step - loss: 0.0763 - val_loss: 0.0720\n",
      "Epoch 57/100\n",
      "2/2 - 0s - 230ms/step - loss: 0.0702 - val_loss: 0.1153\n",
      "Epoch 58/100\n",
      "2/2 - 1s - 261ms/step - loss: 0.0723 - val_loss: 0.0950\n",
      "Epoch 59/100\n",
      "2/2 - 1s - 265ms/step - loss: 0.0661 - val_loss: 0.0756\n",
      "Epoch 60/100\n",
      "2/2 - 1s - 262ms/step - loss: 0.0695 - val_loss: 0.0815\n",
      "Epoch 61/100\n",
      "2/2 - 0s - 236ms/step - loss: 0.0648 - val_loss: 0.1075\n",
      "Epoch 62/100\n",
      "2/2 - 1s - 253ms/step - loss: 0.0653 - val_loss: 0.1178\n",
      "Epoch 63/100\n",
      "2/2 - 1s - 254ms/step - loss: 0.0634 - val_loss: 0.1023\n",
      "Epoch 64/100\n",
      "2/2 - 1s - 256ms/step - loss: 0.0638 - val_loss: 0.1022\n",
      "Epoch 65/100\n",
      "2/2 - 0s - 246ms/step - loss: 0.0639 - val_loss: 0.1190\n",
      "Epoch 66/100\n",
      "2/2 - 0s - 247ms/step - loss: 0.0643 - val_loss: 0.1263\n",
      "Epoch 67/100\n",
      "2/2 - 0s - 240ms/step - loss: 0.0632 - val_loss: 0.1091\n",
      "Epoch 68/100\n",
      "2/2 - 0s - 225ms/step - loss: 0.0633 - val_loss: 0.1076\n",
      "Epoch 69/100\n",
      "2/2 - 0s - 221ms/step - loss: 0.0630 - val_loss: 0.1141\n",
      "Epoch 70/100\n",
      "2/2 - 0s - 214ms/step - loss: 0.0627 - val_loss: 0.1029\n",
      "Epoch 71/100\n",
      "2/2 - 0s - 239ms/step - loss: 0.0628 - val_loss: 0.0961\n",
      "Epoch 72/100\n",
      "2/2 - 0s - 246ms/step - loss: 0.0632 - val_loss: 0.1013\n",
      "Epoch 73/100\n",
      "2/2 - 0s - 227ms/step - loss: 0.0629 - val_loss: 0.1171\n",
      "Epoch 74/100\n",
      "2/2 - 0s - 225ms/step - loss: 0.0640 - val_loss: 0.1095\n",
      "Epoch 75/100\n",
      "2/2 - 0s - 249ms/step - loss: 0.0626 - val_loss: 0.0926\n",
      "Epoch 76/100\n",
      "2/2 - 1s - 264ms/step - loss: 0.0638 - val_loss: 0.0936\n",
      "Epoch 77/100\n",
      "2/2 - 0s - 237ms/step - loss: 0.0629 - val_loss: 0.1088\n",
      "Epoch 78/100\n",
      "2/2 - 0s - 226ms/step - loss: 0.0631 - val_loss: 0.1131\n",
      "Epoch 79/100\n",
      "2/2 - 0s - 233ms/step - loss: 0.0623 - val_loss: 0.1018\n",
      "Epoch 80/100\n",
      "2/2 - 0s - 223ms/step - loss: 0.0623 - val_loss: 0.0989\n",
      "Epoch 81/100\n",
      "2/2 - 0s - 224ms/step - loss: 0.0622 - val_loss: 0.1039\n",
      "Epoch 82/100\n",
      "2/2 - 0s - 236ms/step - loss: 0.0621 - val_loss: 0.1059\n",
      "Epoch 83/100\n",
      "2/2 - 0s - 242ms/step - loss: 0.0619 - val_loss: 0.1012\n",
      "Epoch 84/100\n",
      "2/2 - 0s - 229ms/step - loss: 0.0618 - val_loss: 0.0986\n",
      "Epoch 85/100\n",
      "2/2 - 0s - 232ms/step - loss: 0.0617 - val_loss: 0.0995\n",
      "Epoch 86/100\n",
      "2/2 - 0s - 217ms/step - loss: 0.0615 - val_loss: 0.0962\n",
      "Epoch 87/100\n",
      "2/2 - 0s - 224ms/step - loss: 0.0614 - val_loss: 0.0924\n",
      "Epoch 88/100\n",
      "2/2 - 0s - 228ms/step - loss: 0.0613 - val_loss: 0.0909\n",
      "Epoch 89/100\n",
      "2/2 - 0s - 226ms/step - loss: 0.0611 - val_loss: 0.0898\n",
      "Epoch 90/100\n",
      "2/2 - 0s - 236ms/step - loss: 0.0608 - val_loss: 0.0882\n",
      "Epoch 91/100\n",
      "2/2 - 0s - 229ms/step - loss: 0.0606 - val_loss: 0.0873\n",
      "Epoch 92/100\n",
      "2/2 - 0s - 221ms/step - loss: 0.0603 - val_loss: 0.0872\n",
      "Epoch 93/100\n",
      "2/2 - 0s - 227ms/step - loss: 0.0599 - val_loss: 0.0867\n",
      "Epoch 94/100\n",
      "2/2 - 0s - 240ms/step - loss: 0.0596 - val_loss: 0.0843\n",
      "Epoch 95/100\n",
      "2/2 - 0s - 226ms/step - loss: 0.0591 - val_loss: 0.0816\n",
      "Epoch 96/100\n",
      "2/2 - 0s - 237ms/step - loss: 0.0587 - val_loss: 0.0765\n",
      "Epoch 97/100\n",
      "2/2 - 1s - 327ms/step - loss: 0.0584 - val_loss: 0.0735\n",
      "Epoch 98/100\n",
      "2/2 - 0s - 242ms/step - loss: 0.0580 - val_loss: 0.0724\n",
      "Epoch 99/100\n",
      "2/2 - 0s - 232ms/step - loss: 0.0579 - val_loss: 0.0778\n",
      "Epoch 100/100\n",
      "2/2 - 0s - 232ms/step - loss: 0.0575 - val_loss: 0.0883\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 433ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "未来20天预测汇率：\n",
      "[0.00531634 0.00531851 0.00532172 0.00532584 0.0053306  0.0053357\n",
      " 0.00534083 0.00534573 0.00535022 0.00535415 0.00535742 0.00535998\n",
      " 0.0053618  0.00536285 0.0053632  0.00536288 0.00536193 0.00536042\n",
      " 0.00535843 0.00535601]\n",
      "------------------------------\n",
      "预测 SGD 对人民币的汇率：\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rprp/.local/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "\n",
    "def predict(name:str):\n",
    "    #CNY_HKD_to_exchange_rate.csv\n",
    "    model = CurrencyLSTMModel(currency_name=name, data_path=f\"CNY_{name}_to_exchange_rate.csv\", look_back=100)\n",
    "    model.train()\n",
    "    model.save(f\"{name}_model.keras\")\n",
    "    model.load(f\"{name}_model.keras\")\n",
    "    joblib.dump(model.scaler, 'scaler.save')\n",
    "    model.scaler = joblib.load('scaler.save')\n",
    "    csvname = f\"CNY_{name}_to_exchange_rate.csv\"\n",
    "    # 读取原始数据\n",
    "    dataset = pd.read_csv(csvname, header=0, index_col=0)\n",
    "    values = dataset.values.astype('float32')\n",
    "\n",
    "\n",
    "    # 归一化\n",
    "    scaled = model.scaler.transform(values)\n",
    "\n",
    "    # 取最近100天\n",
    "    last_100 = scaled[-100:]  # shape: (100, 1)\n",
    "\n",
    "    # 递归预测未来20天\n",
    "    future_steps = 20\n",
    "    future_preds = []\n",
    "\n",
    "    input_seq = last_100.reshape((1, 100, 1))\n",
    "\n",
    "    for _ in range(future_steps):\n",
    "        yhat = model.predict(input_seq)\n",
    "        future_preds.append(yhat[0, 0])\n",
    "        # 更新输入序列：去掉最早一天，加上最新预测\n",
    "        new_input = np.append(input_seq[0, 1:, 0], yhat[0, 0])\n",
    "        input_seq = new_input.reshape((1, 100, 1))\n",
    "\n",
    "    # 反归一化\n",
    "    future_preds = np.array(future_preds).reshape(-1, 1)\n",
    "    future_real = model.scaler.inverse_transform(future_preds)\n",
    "    print(\"未来20天预测汇率：\")\n",
    "    print(future_real.flatten())\n",
    "\n",
    "targets = [\"JPY\", \"KRW\", \"SGD\", \"THB\", \"HKD\", \"MYR\"]  # 多个目标货币\n",
    "for target in targets:\n",
    "    print(f\"预测 {target} 对人民币的汇率：\")\n",
    "    predict(target)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a202c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
